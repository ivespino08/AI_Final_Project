This model maps:
    state_vector (length = state_dim)
    → Q-values (length = action_dim)

- we used a neuro netweork that is able to :
Q-learning with replay buffer (DQN-style)
Deep SARSA
Policy Gradient (if adjusted)
Actor-Critic (with modifications)
PPO (optional variation)

--------------------------------

Why 128 neurons?
    - Industry standard for small RL problems.
    - Large enough to learn, small enough to train quickly.
    - Stable, commonly used in DQN implementations.
     - not to harsh on CPU/GPU
    - It is popular in real word using between 64-128 neurons
    - it gives us universal approximation
        -A neural network with at least one sufficiently wide hidden layer can approximate nonlinear functions 128 is comfortably wide.

Why 2 hidden layers?
    - One layer works, but two layers learn faster and more reliably.
    - DQN architectures in research often use 2 layers.

Why ReLU?
    - Best activation for RL networks.
    - Avoids vanishing gradients.
    - Standard in DQN, PPO, actor-critic, etc.

Why output = action_dim?
    - DQN networks must output one Q-value per action.
    - Example: CartPole has 2 actions → output = [Q_left, Q_right].

Project Requirements check?
----------------------------------------
It satisfies:

✔ Function Approximation requirement
✔ Works with Gymnasium (CartPole)
✔ Supports DQN updates and replay buffer
✔ Matches allowed RL techniques listed in professor's PDF
✔ Produces Q-values for RL decision-making

